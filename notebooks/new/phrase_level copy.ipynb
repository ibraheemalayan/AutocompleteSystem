{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 07:16:02.141421: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-22 07:16:02.876669: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 07:16:03.903794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19414 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "tf.device(\"/GPU:0\")\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Generate input sequences\n",
    "MAX_SEQUENCE_LENGTH = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove emojis and special characters from Arabic text\n",
    "def ar_remove_emojis_and_special_characters(text):\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "    # Remove special characters (retain Arabic letters and Arabic numbers)\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF0-9\\s]', '', text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Preprocessing pipeline\n",
    "def ar_preprocess_pipeline(data: str) -> 'list':\n",
    "    \n",
    "    # Split by newline character\n",
    "    sentences = data.replace('.', '\\n').split('\\n')\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = ar_remove_emojis_and_special_characters(sentences[i])\n",
    "        sentences[i] = sentences[i].strip()\n",
    "    \n",
    "    # Drop empty sentences\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    \n",
    "    # Tokenization\n",
    "    tokenized = []\n",
    "    for sentence in sentences:\n",
    "        tokenized.append(sentence)\n",
    "    return tokenized\n",
    "\n",
    "def ar_preprocess(text):\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF0-9\\s]', '', text)\n",
    "    return text.split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import keras\n",
    "\n",
    "def predict_top_five_words(model, tokenizer, seed_text):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=MAX_SEQUENCE_LENGTH, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    top_five_indexes = np.argsort(predicted[0])[::-1][:5]\n",
    "    top_five_words = []\n",
    "    for index in top_five_indexes:\n",
    "        for word, idx in tokenizer.word_index.items():\n",
    "            if idx == index:\n",
    "                top_five_words.append(word)\n",
    "                break\n",
    "    return top_five_words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def predict_and_display_top_five_words(seed_text, model, tokenizer):\n",
    "\n",
    "#     top_five_words = predict_top_five_words(model, tokenizer, seed_text)\n",
    "#     heading_app = f\"<h1>Sentence AutoCompletion App With Five Outputs</h1>\"\n",
    "#     output_text = f\"<ul>{''.join([f'<li>{seed_text} {word}</li>' for word in top_five_words])}</ul>\"\n",
    "#     javascript_code = f\"\"\"\n",
    "#     <script>\n",
    "#         var newWindow = window.open(\"\", \"_blank\");\n",
    "#         newWindow.document.write('<html><head><title>Top Five Words</title></head><body>{heading_app} <br> <hr> {output_text}</body></html>');\n",
    "#     </script>\n",
    "#     \"\"\"\n",
    "#     return HTML(javascript_code)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def update_model_with_feedback(model, tokenizer, feedback_text, seed_text, learning_rate=0.001, batch_size=32, epochs=1):\n",
    "    # Tokenize the feedback text\n",
    "    sequence = tokenizer.texts_to_sequences([feedback_text])[0]\n",
    "\n",
    "    # Tokenize the seed text\n",
    "    seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "    # Prepare input sequence and target\n",
    "    input_sequence = pad_sequences([seed_sequence], maxlen=len(seed_sequence), padding='pre')\n",
    "    target = tf.keras.utils.to_categorical([sequence[-1]], num_classes=len(tokenizer.word_index) + 1)\n",
    "\n",
    "    # Compile the model with appropriate optimizer and loss\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate))\n",
    "\n",
    "    # Train the model with mini-batch training\n",
    "    model.fit(input_sequence, target, batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "\n",
    "    # Optionally, return history for monitoring training progress or validation loss\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#Define custom metrics\n",
    "# @keras.saving.register_keras_serializable()\n",
    "# def top_3_accuracy(y_true, y_pred):\n",
    "#     return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
    "\n",
    "# @keras.saving.register_keras_serializable()\n",
    "# def top_5_accuracy(y_true, y_pred):\n",
    "#     return top_k_categorical_accuracy(y_true, y_pred, k=5)\n",
    "\n",
    "def mean_reciprocal_rank(y_true, y_pred):\n",
    "    y_true = tf.argmax(y_true, axis=1)\n",
    "    y_pred = tf.argsort(y_pred, axis=1, direction='DESCENDING')\n",
    "    ranks = tf.where(tf.equal(y_pred, tf.expand_dims(y_true, axis=1)))[:, 1] + 1\n",
    "    rr = tf.reduce_mean(1.0 / tf.cast(ranks, tf.float32))\n",
    "    return rr\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def perplexity(y_true, y_pred):\n",
    "    cross_entropy = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    perplexity = tf.exp(cross_entropy)\n",
    "    return tf.reduce_mean(perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = defaultdict(TrieNode)\n",
    "        self.is_end_of_word = False\n",
    "        self.frequency = 0\n",
    "\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, word, frequency=1):\n",
    "        node = self.root\n",
    "        for char in word:\n",
    "            node = node.children[char]\n",
    "        node.is_end_of_word = True\n",
    "        node.frequency += frequency\n",
    "\n",
    "    def search(self, prefix):\n",
    "        node = self.root\n",
    "        for char in prefix:\n",
    "            if char not in node.children:\n",
    "                return None\n",
    "            node = node.children[char]\n",
    "        return node\n",
    "\n",
    "    def autocomplete(self, prefix):\n",
    "        node = self.search(prefix)\n",
    "        if not node:\n",
    "            return []\n",
    "\n",
    "        results = []\n",
    "        self._dfs(node, prefix, results)\n",
    "        results.sort(key=lambda x: -x[1])  # Sort by frequency\n",
    "        return [word for word, freq in results]\n",
    "\n",
    "    def _dfs(self, node, prefix, results):\n",
    "        if node.is_end_of_word:\n",
    "            results.append((prefix, node.frequency))\n",
    "        for char, next_node in node.children.items():\n",
    "            self._dfs(next_node, prefix + char, results)\n",
    "\n",
    "    def update(self, word, frequency=1):\n",
    "        self.insert(word, frequency)\n",
    "\n",
    "\n",
    "\n",
    "def build_trie_with_frequency(data):\n",
    "    trie = Trie()\n",
    "    frequency_dict = defaultdict(int)\n",
    "    for query in data:\n",
    "        frequency_dict[query] += 1\n",
    "    for query, freq in frequency_dict.items():\n",
    "        trie.insert(query, freq)\n",
    "    return trie\n",
    "\n",
    "\n",
    "def save_trie(trie, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(trie, file)\n",
    "\n",
    "\n",
    "\n",
    "def load_trie(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # \n",
    "# import marisa_trie\n",
    "# trie = marisa_trie.Trie([u'key1', u'key2', u'key12'])\n",
    "\n",
    "# trie.prefixes(u'key12') # Find all trie keys which are prefixes of a given key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trie.keys(u'key1') # Find all trie keys which start with a given prefix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the filename\n",
    "input_file = '../arabic_data.txt'\n",
    "\n",
    "# Read the contents of the file\n",
    "with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "    ar_data = infile.read()\n",
    "\n",
    "\n",
    "ar_tokens = ar_preprocess(ar_data)\n",
    "ar_trie = build_trie_with_frequency(ar_tokens)\n",
    "save_trie(ar_trie, 'ar_trie.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import everygrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100923"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ar_data = ar_data[:200000]\n",
    "\n",
    "# Tokenize words\n",
    "tokenized_sentences = ar_preprocess_pipeline(ar_data)\n",
    "\n",
    "ar_tokenizer = Tokenizer(oov_token='<oov>')\n",
    "ar_tokenizer.fit_on_texts(tokenized_sentences)\n",
    "total_words = len(ar_tokenizer.word_index) + 1\n",
    "# tokenizer.word_counts\n",
    "# tokenizer.word_index\n",
    "\n",
    "\n",
    "\n",
    "input_sequences = []\n",
    "for line in tokenized_sentences:\n",
    "    token_list = ar_tokenizer.texts_to_sequences([line])[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # use everygrams\n",
    "    input_sequences.extend(list(everygrams(token_list, max_len=MAX_SEQUENCE_LENGTH, min_len=2)))\n",
    "    \n",
    "    \n",
    "    # for i in range(1, len(token_list)):\n",
    "    #     n_gram_sequence = token_list[:i + 1]\n",
    "    #     input_sequences.append(n_gram_sequence)\n",
    "len(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_sequences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pad sequences\n",
    "max_sequence_len = MAX_SEQUENCE_LENGTH\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn them into tensors\n",
    "X = input_sequences[:,:-1]\n",
    "\n",
    "# turn labels (input_sequences[:,-2:-1]) into numpy array\n",
    "labels = input_sequences[:,-1]\n",
    "\n",
    "labels_encoded = keras.utils.to_categorical(labels, num_classes=total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0, 1618],\n",
       "       [   0, 1618,  722],\n",
       "       [1618,  722,    3],\n",
       "       ...,\n",
       "       [   0,    0, 1811],\n",
       "       [   0, 1811,   18],\n",
       "       [   0,    0,   18]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, labels_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# turn them into tensors\n",
    "# X_train = tf.convert_to_tensor(X_train)\n",
    "# X_val = tf.convert_to_tensor(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# y_train = tf.convert_to_tensor(y_train)\n",
    "# y_val = tf.convert_to_tensor(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = tf.data.Dataset.from_tensors((X_train, y_train))\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "# train_dataset.save('train_dataset')\n",
    "# val_dataset.save('val_dataset')\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.4, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# del labels\n",
    "# del labels_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del tokenized_sentences\n",
    "# del input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       y_train:  6.6 GiB\n",
      "                         y_val:  2.8 GiB\n",
      "               input_sequences:  1.5 MiB\n",
      "                     ar_tokens: 991.9 KiB\n",
      "                       X_train: 828.0 KiB\n",
      "                       ar_data: 390.7 KiB\n",
      "                         X_val: 354.9 KiB\n",
      "           tokenized_sentences:  3.2 KiB\n",
      "                           _i6:  3.0 KiB\n",
      "                           _i5:  2.0 KiB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in list(\n",
    "                          locals().items())), key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12628"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70646, 3)\n",
      "(30277, 3)\n",
      "(70646, 12628)\n",
      "(30277, 12628)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "# X_train: (326825, 4)\n",
    "# X_val: (140069, 4)\n",
    "# y_train: (326825,)\n",
    "# y_val: (140069,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.3'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19000000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,616,384</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">788,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12628</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,478,164</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_8 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m2048\u001b[0m, \u001b[38;5;34m19000000\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m1,616,384\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_8 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;34m2048\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m788,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;34m2048\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m2048\u001b[0m, \u001b[38;5;34m12628\u001b[0m)          │     \u001b[38;5;34m6,478,164\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,883,028</span> (33.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,883,028\u001b[0m (33.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,883,028</span> (33.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,883,028\u001b[0m (33.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "total_words = 35000\n",
    "\n",
    "# Define your model\n",
    "ar_model = keras.Sequential()\n",
    "ar_model.add(keras.layers.InputLayer(batch_size=2048, shape=(19000000, )))\n",
    "ar_model.add(keras.layers.Embedding(input_dim=total_words, output_dim=128, input_length=4))\n",
    "ar_model.add(keras.layers.Bidirectional(keras.layers.LSTM(256)))\n",
    "ar_model.add(keras.layers.Dropout(0.2))\n",
    "ar_model.add(keras.layers.Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Compile the model with multiple metrics\n",
    "adam = keras.optimizers.Adam(learning_rate=0.01)\n",
    "ar_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=adam,\n",
    "              metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5), keras_nlp.metrics.Perplexity])\n",
    "ar_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export TF_GPU_ALLOCATOR=cuda_malloc_async\n",
    "!export TF_GPU_ALLOCATOR=cuda_malloc_async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None, 12628), output.shape=(None, 12628)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mar_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras_nlp/src/metrics/perplexity.py:142\u001b[0m, in \u001b[0;36mPerplexity.update_state\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    138\u001b[0m         sample_weight \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mmultiply(mask, sample_weight)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Calculate the Cross Entropy Loss.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m crossentropy_value \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcast(\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_crossentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    144\u001b[0m )  \u001b[38;5;66;03m# scalar\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Divide the loss by the number of non-masked tokens\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None, 12628), output.shape=(None, 12628)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "history = ar_model.fit(X_train, y_train, \n",
    "                     batch_size=4,\n",
    "                       epochs=3, validation_data=(X_val, y_val), verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Top-3 Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['top_3_accuracy'], label='Training Top-3 Accuracy')\n",
    "plt.plot(history.history['val_top_3_accuracy'], label='Validation Top-3 Accuracy')\n",
    "plt.title('Training and Validation Top-3 Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Top-3 Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Top-5 Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['top_5_accuracy'], label='Training Top-5 Accuracy')\n",
    "plt.plot(history.history['val_top_5_accuracy'], label='Validation Top-5 Accuracy')\n",
    "plt.title('Training and Validation Top-5 Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Top-5 Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Perplexity\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['perplexity'], label='Training Perplexity')\n",
    "plt.plot(history.history['val_perplexity'], label='Validation Perplexity')\n",
    "plt.title('Training and Validation Perplexity')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model architecture to JSON\n",
    "\n",
    "ar_model.save('ar_model.keras')\n",
    "\n",
    "# ar_model_json = ar_model.to_json()\n",
    "# with open(\"ar_model.json\", \"w\") as json_file:\n",
    "#     json_file.write(ar_model_json)\n",
    "\n",
    "# # Save the weights to HDF5\n",
    "# ar_model.save_weights(\"ar_model.weights.h5\")\n",
    "\n",
    "# Save the tokenizer to a file\n",
    "with open('ar_tokenizer.pkl', 'wb') as handle:\n",
    "    pickle.dump(ar_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_ar_trie = load_trie('ar_trie.pkl')\n",
    "loaded_ar_trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_ar_trie.autocomplete('ا')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_top_five_words(ar_model, ar_tokenizer, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = 'صناعة ألماس شديد الشبه بالألماس الطبيعي بواسطة الليزر وبتكلفة قل'\n",
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token_list = ar_tokenizer.texts_to_sequences([seed_text])[0]\n",
    "token_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_WORDS_IN_CONTEXT = 20 - 1\n",
    "\n",
    "padded_token_list = pad_sequences([token_list], maxlen=MAX_WORDS_IN_CONTEXT, padding='pre')\n",
    "padded_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted = ar_model.predict(padded_token_list, verbose=0)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(predicted[::-1][:10])\n",
    "print(predicted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_indexes = np.argsort(predicted)[::-1][:20]\n",
    "\n",
    "top_words = []\n",
    "for index in top_indexes:\n",
    "    for word, idx in ar_tokenizer.word_index.items():\n",
    "        if idx == index:\n",
    "            top_words.append(word)\n",
    "            break\n",
    "\n",
    "top_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
